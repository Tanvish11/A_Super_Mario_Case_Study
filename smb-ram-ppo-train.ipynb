{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d63118",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f9b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym \n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym_utils import SMBRamWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3dac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deher\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdffe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cropping size\n",
    "x0 = 0\n",
    "x1 = 16\n",
    "y0 = 0\n",
    "y1 = 13\n",
    "n_stack = 4  #take 5\n",
    "n_skip = 4     #5\n",
    "\n",
    "env_wrap = SMBRamWrapper(env, [x0, x1, y0, y1], n_stack=n_stack, n_skip=n_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1018d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deher\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\deher\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\deher\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m      5\u001b[0m     state \u001b[38;5;241m=\u001b[39m env_wrap\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 6\u001b[0m state, reward, done, info \u001b[38;5;241m=\u001b[39m env_wrap\u001b[38;5;241m.\u001b[39mstep(env_wrap\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n",
      "File \u001b[1;32mc:\\Users\\deher\\anaconda3\\Lib\\site-packages\\gym\\core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\deher\\anaconda3\\Lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_map[action])\n",
      "File \u001b[1;32mc:\\Users\\deher\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "# test env_wrap\n",
    "done = True\n",
    "for i in range(150):\n",
    "    if done:\n",
    "        state = env_wrap.reset()\n",
    "    state, reward, done, info = env_wrap.step(env_wrap.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be737c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 16, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc903d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply other wrapper functions\n",
    "env_wrap = Monitor(env_wrap)  # for tensorboard log\n",
    "env_wrap = DummyVecEnv([lambda: env_wrap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f25b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "# Save intermediate models\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, \n",
    "                 starting_steps=0, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.starting_steps = starting_steps\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls + int(self.starting_steps)))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U need to modify these two directories before training a new model\n",
    "MODEL_DIR = '/model/direct'       \n",
    "LOG_DIR = '/log/newlog'                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_wrap, verbose=1, learning_rate=linear_schedule(3e-4), tensorboard_log=LOG_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1e5, starting_steps=0, save_path=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b8e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_start = time.time()            #train the model\n",
    "\n",
    "model.learn(total_timesteps=10e6, callback=callback)\n",
    "\n",
    "t_elapsed = time.time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3773e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Wall time: {} s'.format(round(t_elapsed, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ef893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = os.path.join(MODEL_DIR, 'SAVED_MODEL_NAME')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd36cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "MODEL_DIR = './models/SAVED_MODEL_NAME'    #Add your directory location\n",
    "LOG_DIR = './logs/SAVED_LOG_DIR'\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, 'SAVED_MODEL_NAME')\n",
    "model = PPO.load(model_path, env=env_wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b606aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_NAME = 'PPO_5'\n",
    "TB_LOG = os.path.join(LOG_DIR, LOG_NAME)\n",
    "\n",
    "!tensorboard --logdir={TB_LOG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1\n",
    "\n",
    "for episode in range(1, episode+1):\n",
    "    states = env_wrap.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env_wrap.render()\n",
    "        action, _ = model.predict(states, deterministic=True)\n",
    "        states, reward, done, info = env_wrap.step(action)\n",
    "        score += reward\n",
    "        time.sleep(0.01)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
